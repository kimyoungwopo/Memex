import { useEffect, useState, useCallback, useRef } from "react"

// Chrome Built-in AI ìµœì‹  íƒ€ìž… ì •ì˜ (2026 Prompt API Spec)
// https://github.com/webmachinelearning/prompt-api

type LanguageModelAvailability = "available" | "downloadable" | "downloading" | "unavailable"

interface LanguageModelParams {
  defaultTemperature: number
  maxTemperature: number
  defaultTopK: number
  maxTopK: number
}

interface LanguageModelPrompt {
  role: "system" | "user" | "assistant"
  content: string | LanguageModelContent[]
}

interface LanguageModelContent {
  type: "text" | "image" | "audio"
  value: string | Blob | ImageData | ImageBitmap | AudioBuffer | BufferSource
}

interface LanguageModelExpectedIO {
  type?: "text" | "image" | "audio"
  languages?: string[]
}

interface LanguageModelCreateOptions {
  initialPrompts?: LanguageModelPrompt[]
  temperature?: number
  topK?: number
  expectedInputs?: LanguageModelExpectedIO[]
  expectedOutputs?: LanguageModelExpectedIO[]
  signal?: AbortSignal
  monitor?: (monitor: LanguageModelDownloadMonitor) => void
}

interface LanguageModelDownloadMonitor extends EventTarget {
  addEventListener(
    type: "downloadprogress",
    listener: (event: LanguageModelDownloadProgressEvent) => void
  ): void
}

interface LanguageModelDownloadProgressEvent extends Event {
  loaded: number
}

interface LanguageModelPromptOptions {
  signal?: AbortSignal
  responseConstraint?: object | RegExp
}

interface LanguageModelSession {
  prompt: (input: string | LanguageModelPrompt[], options?: LanguageModelPromptOptions) => Promise<string>
  promptStreaming: (input: string | LanguageModelPrompt[], options?: LanguageModelPromptOptions) => ReadableStream<string>
  append: (prompts: LanguageModelPrompt[]) => Promise<void>
  measureInputUsage: (input: string | LanguageModelPrompt[]) => Promise<number>
  clone: (options?: { signal?: AbortSignal }) => Promise<LanguageModelSession>
  destroy: () => void
  readonly inputUsage: number
  readonly inputQuota: number
  addEventListener(type: "quotaoverflow", listener: () => void): void
}

interface LanguageModelAPI {
  availability: (options?: {
    expectedInputs?: LanguageModelExpectedIO[]
    expectedOutputs?: LanguageModelExpectedIO[]
  }) => Promise<LanguageModelAvailability>
  params: () => Promise<LanguageModelParams | null>
  create: (options?: LanguageModelCreateOptions) => Promise<LanguageModelSession>
}

export type AIStatus = "loading" | "ready" | "downloading" | "error" | "unsupported"

// AI API ì°¾ê¸° (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
const getLanguageModel = (): LanguageModelAPI | null => {
  // 1. ì „ì—­ LanguageModel ê°ì²´ í™•ì¸ (ìµœì‹  ìŠ¤íŽ™)
  // @ts-ignore
  if (typeof LanguageModel !== "undefined") {
    // @ts-ignore
    return LanguageModel as LanguageModelAPI
  }
  // 2. window.ai.languageModel í™•ì¸ (ë ˆê±°ì‹œ í˜¸í™˜)
  // @ts-ignore
  if (typeof window !== "undefined" && window.ai?.languageModel) {
    // @ts-ignore
    return window.ai.languageModel as LanguageModelAPI
  }
  // 3. self.ai.languageModel í™•ì¸
  // @ts-ignore
  if (typeof self !== "undefined" && self.ai?.languageModel) {
    // @ts-ignore
    return self.ai.languageModel as LanguageModelAPI
  }
  return null
}

export const useGemini = () => {
  const [status, setStatus] = useState<AIStatus>("loading")
  const [downloadProgress, setDownloadProgress] = useState<number>(0)
  const sessionRef = useRef<LanguageModelSession | null>(null)
  const abortControllerRef = useRef<AbortController | null>(null)

  useEffect(() => {
    const initModel = async () => {
      try {
        abortControllerRef.current = new AbortController()

        // 1. API ì¡´ìž¬ ì—¬ë¶€ í™•ì¸
        const languageModel = getLanguageModel()

        console.log("ðŸ” Checking AI API...")

        if (!languageModel) {
          console.error("âŒ Chrome AI API not found. Please check:")
          console.error("1. Use Chrome Canary or Dev (version 131+)")
          console.error("2. Enable chrome://flags/#optimization-guide-on-device-model")
          console.error("3. Enable chrome://flags/#prompt-api-for-gemini-nano")
          setStatus("error")
          return
        }

        console.log("âœ… AI API found")

        // 2. ëª¨ë¸ ê°€ìš©ì„± í™•ì¸ (ìµœì‹  API: availability())
        const availability = await languageModel.availability()
        console.log("ðŸ“Š Model Availability:", availability)

        if (availability === "unavailable") {
          console.error("âŒ Model not available on this device")
          setStatus("unsupported")
          return
        }

        if (availability === "downloadable" || availability === "downloading") {
          setStatus("downloading")
        }

        // 3. ëª¨ë¸ íŒŒë¼ë¯¸í„° í™•ì¸ (ì„ íƒì‚¬í•­)
        const params = await languageModel.params()
        if (params) {
          console.log("ðŸ“Š Model Params:", params)
        }

        console.log("ðŸš€ Creating AI session...")

        // 4. ì„¸ì…˜ ìƒì„± (ìµœì‹  API ìŠ¤íŽ™)
        const newSession = await languageModel.create({
          // initialPromptsë¡œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
          initialPrompts: [
            {
              role: "system",
              content:
                "ë‹¹ì‹ ì€ 'Memex'ë¼ëŠ” ì´ë¦„ì˜ ìœ ëŠ¥í•œ ë¡œì»¬ AI ë¹„ì„œìž…ë‹ˆë‹¤. " +
                "ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ëŒ€í•´ í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”. " +
                "ë‹µë³€ì€ ëª…í™•í•˜ê³  ì¹œì ˆí•´ì•¼ í•˜ë©°, ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤."
            }
          ],
          // AbortSignal ì „ë‹¬
          signal: abortControllerRef.current.signal,
          // ë‹¤ìš´ë¡œë“œ ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§
          monitor: (monitor) => {
            monitor.addEventListener("downloadprogress", (event) => {
              const progress = Math.round(event.loaded * 100)
              console.log(`ðŸ“¥ Download progress: ${progress}%`)
              setDownloadProgress(progress)
            })
          }
        })

        sessionRef.current = newSession
        setStatus("ready")

        // ì„¸ì…˜ ì •ë³´ ë¡œê¹…
        console.log(`âœ… Session ready. Usage: ${newSession.inputUsage}/${newSession.inputQuota} tokens`)

        // Quota overflow ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ
        newSession.addEventListener("quotaoverflow", () => {
          console.warn("âš ï¸ Context window exceeded; old messages may be removed")
        })

      } catch (e) {
        console.error("AI Initialization Failed:", e)
        setStatus("error")
      }
    }

    initModel()

    // Cleanup: ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì„¸ì…˜ ì •ë¦¬
    return () => {
      if (abortControllerRef.current) {
        abortControllerRef.current.abort()
      }
      if (sessionRef.current) {
        try {
          sessionRef.current.destroy()
        } catch (e) {
          // ignore error
        }
      }
    }
  }, [])

  // ë‹µë³€ ìƒì„± í•¨ìˆ˜
  const generate = useCallback(
    async (input: string, options?: { signal?: AbortSignal }) => {
      if (!sessionRef.current || status !== "ready") {
        throw new Error("AI ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
      }
      try {
        const response = await sessionRef.current.prompt(input, {
          signal: options?.signal
        })
        return response
      } catch (e) {
        if (e instanceof Error && e.name === "AbortError") {
          return "ìš”ì²­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
        }
        console.error("Generation Error:", e)
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
      }
    },
    [status]
  )

  // ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± í•¨ìˆ˜
  const generateStream = useCallback(
    (input: string, options?: { signal?: AbortSignal }) => {
      if (!sessionRef.current || status !== "ready") {
        throw new Error("AI ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
      }
      return sessionRef.current.promptStreaming(input, {
        signal: options?.signal
      })
    },
    [status]
  )

  // í† í° ì‚¬ìš©ëŸ‰ ì¸¡ì •
  const measureTokens = useCallback(
    async (input: string) => {
      if (!sessionRef.current) return 0
      return await sessionRef.current.measureInputUsage(input)
    },
    []
  )

  // í˜„ìž¬ ì„¸ì…˜ ì •ë³´
  const getSessionInfo = useCallback(() => {
    if (!sessionRef.current) return null
    return {
      inputUsage: sessionRef.current.inputUsage,
      inputQuota: sessionRef.current.inputQuota
    }
  }, [])

  return {
    status,
    downloadProgress,
    generate,
    generateStream,
    measureTokens,
    getSessionInfo
  }
}
