/**
 * Embeddings Module - Transformers.jsë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”©
 *
 * ëª¨ë¸: Xenova/all-MiniLM-L6-v2 (384ì°¨ì›, ë¹ ë¥´ê³  ê°€ë²¼ì›€)
 * ìš©ë„: í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì‹œë§¨í‹± ê²€ìƒ‰ ì§€ì›
 */

import { pipeline, env, type FeatureExtractionPipeline } from "@xenova/transformers"

// Chrome Extension í™˜ê²½ ì„¤ì • (ONNX Runtime í˜¸í™˜)
env.allowLocalModels = false
env.useBrowserCache = true
// WASM ë°±ì—”ë“œë§Œ ì‚¬ìš© (WebGPU/WebGL ë¹„í™œì„±í™”)
env.backends = {
  onnx: {
    wasm: {
      numThreads: 1,
    },
  },
}
// CDNì—ì„œ WASM íŒŒì¼ ë¡œë“œ
env.allowRemoteModels = true

// ì‹±ê¸€í†¤ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤
let embeddingPipeline: FeatureExtractionPipeline | null = null
let isLoading = false
let loadPromise: Promise<FeatureExtractionPipeline> | null = null

// ëª¨ë¸ ì„¤ì •
const MODEL_NAME = "Xenova/all-MiniLM-L6-v2"
export const EMBEDDING_DIMENSION = 384

// í…ìŠ¤íŠ¸ ì²­í‚¹ ì„¤ì •
const CHUNK_SIZE = 500 // ì²­í¬ë‹¹ ìµœëŒ€ ë¬¸ì ìˆ˜
const CHUNK_OVERLAP = 50 // ì²­í¬ ê°„ ì˜¤ë²„ë©

export type EmbeddingStatus = "idle" | "loading" | "ready" | "error"

/**
 * ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
 */
export async function initEmbeddings(): Promise<FeatureExtractionPipeline> {
  // ì´ë¯¸ ë¡œë“œëœ ê²½ìš°
  if (embeddingPipeline) {
    return embeddingPipeline
  }

  // ë¡œë”© ì¤‘ì¸ ê²½ìš° ê¸°ì¡´ Promise ë°˜í™˜
  if (isLoading && loadPromise) {
    return loadPromise
  }

  isLoading = true
  console.log("ğŸ§  Loading embedding model:", MODEL_NAME)

  loadPromise = pipeline("feature-extraction", MODEL_NAME, {
    // @ts-ignore - Chrome Extension í™˜ê²½ ìµœì í™”
    progress_callback: (progress: { status: string; progress?: number }) => {
      if (progress.status === "progress" && progress.progress) {
        console.log(`ğŸ“¥ Model loading: ${Math.round(progress.progress)}%`)
      }
    },
  }).then((pipe) => {
    embeddingPipeline = pipe as FeatureExtractionPipeline
    isLoading = false
    console.log("âœ… Embedding model loaded successfully")
    return embeddingPipeline
  }).catch((error) => {
    isLoading = false
    console.error("âŒ Failed to load embedding model:", error)
    throw error
  })

  return loadPromise
}

/**
 * í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
 */
export async function generateEmbedding(text: string): Promise<number[]> {
  const pipe = await initEmbeddings()

  // í…ìŠ¤íŠ¸ ì •ê·œí™”
  const normalizedText = text
    .replace(/\s+/g, " ")
    .trim()
    .slice(0, 512) // ëª¨ë¸ ìµœëŒ€ í† í° ìˆ˜ ê³ ë ¤

  const output = await pipe(normalizedText, {
    pooling: "mean",
    normalize: true,
  })

  // Float32Arrayë¥¼ ì¼ë°˜ ë°°ì—´ë¡œ ë³€í™˜
  return Array.from(output.data as Float32Array)
}

/**
 * ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
 */
export function chunkText(text: string): string[] {
  const chunks: string[] = []
  const sentences = text.split(/(?<=[.!?ã€‚])\s+/)

  let currentChunk = ""

  for (const sentence of sentences) {
    if ((currentChunk + sentence).length <= CHUNK_SIZE) {
      currentChunk += (currentChunk ? " " : "") + sentence
    } else {
      if (currentChunk) {
        chunks.push(currentChunk)
      }
      // ì˜¤ë²„ë© ì ìš©
      const words = currentChunk.split(" ")
      const overlapWords = words.slice(-Math.ceil(CHUNK_OVERLAP / 5))
      currentChunk = overlapWords.join(" ") + " " + sentence
    }
  }

  if (currentChunk.trim()) {
    chunks.push(currentChunk.trim())
  }

  // ì²­í¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
  if (chunks.length === 0 && text.trim()) {
    chunks.push(text.slice(0, CHUNK_SIZE))
  }

  return chunks
}

/**
 * ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì²­í¬ì˜ ì„ë² ë”© ìƒì„±
 */
export async function generateChunkEmbeddings(
  chunks: string[]
): Promise<number[][]> {
  const embeddings: number[][] = []

  for (const chunk of chunks) {
    const embedding = await generateEmbedding(chunk)
    embeddings.push(embedding)
  }

  return embeddings
}

/**
 * ì—¬ëŸ¬ ì„ë² ë”©ì˜ í‰ê·  ê³„ì‚° (ë¬¸ì„œ ì „ì²´ ì„ë² ë”©)
 */
export function averageEmbeddings(embeddings: number[][]): number[] {
  if (embeddings.length === 0) {
    return new Array(EMBEDDING_DIMENSION).fill(0)
  }

  if (embeddings.length === 1) {
    return embeddings[0]
  }

  const avgEmbedding = new Array(EMBEDDING_DIMENSION).fill(0)

  for (const emb of embeddings) {
    for (let i = 0; i < EMBEDDING_DIMENSION; i++) {
      avgEmbedding[i] += emb[i]
    }
  }

  for (let i = 0; i < EMBEDDING_DIMENSION; i++) {
    avgEmbedding[i] /= embeddings.length
  }

  // ì •ê·œí™”
  const norm = Math.sqrt(avgEmbedding.reduce((sum, val) => sum + val * val, 0))
  if (norm > 0) {
    for (let i = 0; i < EMBEDDING_DIMENSION; i++) {
      avgEmbedding[i] /= norm
    }
  }

  return avgEmbedding
}

/**
 * ë‘ ì„ë² ë”© ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
 */
export function cosineSimilarity(a: number[], b: number[]): number {
  let dotProduct = 0
  let normA = 0
  let normB = 0

  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i]
    normA += a[i] * a[i]
    normB += b[i] * b[i]
  }

  normA = Math.sqrt(normA)
  normB = Math.sqrt(normB)

  if (normA === 0 || normB === 0) return 0

  return dotProduct / (normA * normB)
}

/**
 * ì„ë² ë”© ìƒíƒœ í™•ì¸
 */
export function getEmbeddingStatus(): EmbeddingStatus {
  if (embeddingPipeline) return "ready"
  if (isLoading) return "loading"
  return "idle"
}
