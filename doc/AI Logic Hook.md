# AI Logic Hook

> `src/hooks/use-gemini.ts` - Chrome Built-in AI ì´ˆê¸°í™” ë° ì‘ë‹µ ìƒì„± í›…
>
> **API Reference:** https://github.com/webmachinelearning/prompt-api

```typescript
import { useEffect, useState, useCallback, useRef } from "react"

// Chrome Built-in AI ìµœì‹  íƒ€ì… ì •ì˜ (2026 Prompt API Spec)
// https://github.com/webmachinelearning/prompt-api

type LanguageModelAvailability = "available" | "downloadable" | "downloading" | "unavailable"

interface LanguageModelParams {
  defaultTemperature: number
  maxTemperature: number
  defaultTopK: number
  maxTopK: number
}

interface LanguageModelPrompt {
  role: "system" | "user" | "assistant"
  content: string | LanguageModelContent[]
}

interface LanguageModelContent {
  type: "text" | "image" | "audio"
  value: string | Blob | ImageData | ImageBitmap | AudioBuffer | BufferSource
}

interface LanguageModelExpectedIO {
  type?: "text" | "image" | "audio"
  languages?: string[]
}

interface LanguageModelCreateOptions {
  initialPrompts?: LanguageModelPrompt[]
  temperature?: number
  topK?: number
  expectedInputs?: LanguageModelExpectedIO[]
  expectedOutputs?: LanguageModelExpectedIO[]
  signal?: AbortSignal
  monitor?: (monitor: LanguageModelDownloadMonitor) => void
}

interface LanguageModelDownloadMonitor extends EventTarget {
  addEventListener(
    type: "downloadprogress",
    listener: (event: LanguageModelDownloadProgressEvent) => void
  ): void
}

interface LanguageModelDownloadProgressEvent extends Event {
  loaded: number
}

interface LanguageModelPromptOptions {
  signal?: AbortSignal
  responseConstraint?: object | RegExp
}

interface LanguageModelSession {
  prompt: (input: string | LanguageModelPrompt[], options?: LanguageModelPromptOptions) => Promise<string>
  promptStreaming: (input: string | LanguageModelPrompt[], options?: LanguageModelPromptOptions) => ReadableStream<string>
  append: (prompts: LanguageModelPrompt[]) => Promise<void>
  measureInputUsage: (input: string | LanguageModelPrompt[]) => Promise<number>
  clone: (options?: { signal?: AbortSignal }) => Promise<LanguageModelSession>
  destroy: () => void
  readonly inputUsage: number
  readonly inputQuota: number
  addEventListener(type: "quotaoverflow", listener: () => void): void
}

interface LanguageModelAPI {
  availability: (options?: {
    expectedInputs?: LanguageModelExpectedIO[]
    expectedOutputs?: LanguageModelExpectedIO[]
  }) => Promise<LanguageModelAvailability>
  params: () => Promise<LanguageModelParams | null>
  create: (options?: LanguageModelCreateOptions) => Promise<LanguageModelSession>
}

export type AIStatus = "loading" | "ready" | "downloading" | "error" | "unsupported"

// AI API ì°¾ê¸° (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
const getLanguageModel = (): LanguageModelAPI | null => {
  // 1. ì „ì—­ LanguageModel ê°ì²´ í™•ì¸ (ìµœì‹  ìŠ¤í™)
  // @ts-ignore
  if (typeof LanguageModel !== "undefined") {
    // @ts-ignore
    return LanguageModel as LanguageModelAPI
  }
  // 2. window.ai.languageModel í™•ì¸ (ë ˆê±°ì‹œ í˜¸í™˜)
  // @ts-ignore
  if (typeof window !== "undefined" && window.ai?.languageModel) {
    // @ts-ignore
    return window.ai.languageModel as LanguageModelAPI
  }
  // 3. self.ai.languageModel í™•ì¸
  // @ts-ignore
  if (typeof self !== "undefined" && self.ai?.languageModel) {
    // @ts-ignore
    return self.ai.languageModel as LanguageModelAPI
  }
  return null
}

export const useGemini = () => {
  const [status, setStatus] = useState<AIStatus>("loading")
  const [downloadProgress, setDownloadProgress] = useState<number>(0)
  const sessionRef = useRef<LanguageModelSession | null>(null)
  const abortControllerRef = useRef<AbortController | null>(null)

  useEffect(() => {
    const initModel = async () => {
      try {
        abortControllerRef.current = new AbortController()

        // 1. API ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        const languageModel = getLanguageModel()

        console.log("ğŸ” Checking AI API...")

        if (!languageModel) {
          console.error("âŒ Chrome AI API not found. Please check:")
          console.error("1. Use Chrome Canary or Dev (version 131+)")
          console.error("2. Enable chrome://flags/#optimization-guide-on-device-model")
          console.error("3. Enable chrome://flags/#prompt-api-for-gemini-nano")
          setStatus("error")
          return
        }

        console.log("âœ… AI API found")

        // 2. ëª¨ë¸ ê°€ìš©ì„± í™•ì¸ (ìµœì‹  API: availability())
        const availability = await languageModel.availability({
          expectedInputs: [{ type: "text" }],
          expectedOutputs: [{ type: "text" }]
        })
        console.log("ğŸ“Š Model Availability:", availability)

        if (availability === "unavailable") {
          console.error("âŒ Model not available on this device")
          setStatus("unsupported")
          return
        }

        if (availability === "downloadable" || availability === "downloading") {
          setStatus("downloading")
        }

        // 3. ëª¨ë¸ íŒŒë¼ë¯¸í„° í™•ì¸ (ì„ íƒì‚¬í•­)
        const params = await languageModel.params()
        if (params) {
          console.log("ğŸ“Š Model Params:", params)
        }

        console.log("ğŸš€ Creating AI session...")

        // 4. ì„¸ì…˜ ìƒì„± (ìµœì‹  API ìŠ¤í™)
        const newSession = await languageModel.create({
          // initialPromptsë¡œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
          initialPrompts: [
            {
              role: "system",
              content:
                "ë‹¹ì‹ ì€ 'Memex'ë¼ëŠ” ì´ë¦„ì˜ ìœ ëŠ¥í•œ ë¡œì»¬ AI ë¹„ì„œì…ë‹ˆë‹¤. " +
                "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”. " +
                "ë‹µë³€ì€ ëª…í™•í•˜ê³  ì¹œì ˆí•´ì•¼ í•˜ë©°, ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            }
          ],
          // ì…ì¶œë ¥ íƒ€ì… ë° ì–¸ì–´ ì„ ì–¸
          expectedInputs: [{ type: "text", languages: ["ko", "en"] }],
          expectedOutputs: [{ type: "text", languages: ["ko"] }],
          // AbortSignal ì „ë‹¬
          signal: abortControllerRef.current.signal,
          // ë‹¤ìš´ë¡œë“œ ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§
          monitor: (monitor) => {
            monitor.addEventListener("downloadprogress", (event) => {
              const progress = Math.round(event.loaded * 100)
              console.log(`ğŸ“¥ Download progress: ${progress}%`)
              setDownloadProgress(progress)
            })
          }
        })

        sessionRef.current = newSession
        setStatus("ready")

        // ì„¸ì…˜ ì •ë³´ ë¡œê¹…
        console.log(`âœ… Session ready. Usage: ${newSession.inputUsage}/${newSession.inputQuota} tokens`)

        // Quota overflow ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ
        newSession.addEventListener("quotaoverflow", () => {
          console.warn("âš ï¸ Context window exceeded; old messages may be removed")
        })

      } catch (e) {
        console.error("AI Initialization Failed:", e)
        setStatus("error")
      }
    }

    initModel()

    // Cleanup: ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì„¸ì…˜ ì •ë¦¬
    return () => {
      if (abortControllerRef.current) {
        abortControllerRef.current.abort()
      }
      if (sessionRef.current) {
        try {
          sessionRef.current.destroy()
        } catch (e) {
          // ignore error
        }
      }
    }
  }, [])

  // ë‹µë³€ ìƒì„± í•¨ìˆ˜
  const generate = useCallback(
    async (input: string, options?: { signal?: AbortSignal }) => {
      if (!sessionRef.current || status !== "ready") {
        throw new Error("AI ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
      }
      try {
        const response = await sessionRef.current.prompt(input, {
          signal: options?.signal
        })
        return response
      } catch (e) {
        if (e instanceof Error && e.name === "AbortError") {
          return "ìš”ì²­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
        }
        console.error("Generation Error:", e)
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
      }
    },
    [status]
  )

  // ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± í•¨ìˆ˜
  const generateStream = useCallback(
    (input: string, options?: { signal?: AbortSignal }) => {
      if (!sessionRef.current || status !== "ready") {
        throw new Error("AI ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
      }
      return sessionRef.current.promptStreaming(input, {
        signal: options?.signal
      })
    },
    [status]
  )

  // í† í° ì‚¬ìš©ëŸ‰ ì¸¡ì •
  const measureTokens = useCallback(
    async (input: string) => {
      if (!sessionRef.current) return 0
      return await sessionRef.current.measureInputUsage(input)
    },
    []
  )

  // í˜„ì¬ ì„¸ì…˜ ì •ë³´
  const getSessionInfo = useCallback(() => {
    if (!sessionRef.current) return null
    return {
      inputUsage: sessionRef.current.inputUsage,
      inputQuota: sessionRef.current.inputQuota
    }
  }, [])

  return {
    status,
    downloadProgress,
    generate,
    generateStream,
    measureTokens,
    getSessionInfo
  }
}
```

## API ë³€ê²½ì‚¬í•­ (2026 Prompt API Spec)

### ì£¼ìš” ë³€ê²½ì 

| ì´ì „ API | ìµœì‹  API | ì„¤ëª… |
|----------|----------|------|
| `window.ai.languageModel` | `LanguageModel` | ì „ì—­ ê°ì²´ë¡œ ë³€ê²½ (ë ˆê±°ì‹œ í˜¸í™˜ ìœ ì§€) |
| `capabilities()` | `availability()` | ë©”ì„œë“œëª… ë³€ê²½ |
| `"readily"` | `"available"` | ë°˜í™˜ê°’ ë³€ê²½ |
| `"after-download"` | `"downloadable"` | ë°˜í™˜ê°’ ë³€ê²½ |
| `"no"` | `"unavailable"` | ë°˜í™˜ê°’ ë³€ê²½ |
| `systemPrompt` | `initialPrompts[{role:"system"}]` | ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°©ì‹ ë³€ê²½ |

### ìƒˆë¡œìš´ ê¸°ëŠ¥

- **`params()`**: ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¡°íšŒ (temperature, topK ê¸°ë³¸ê°’/ìµœëŒ€ê°’)
- **`measureInputUsage()`**: í† í° ì‚¬ìš©ëŸ‰ ì¸¡ì •
- **`inputUsage` / `inputQuota`**: ì„¸ì…˜ í† í° ì‚¬ìš©ëŸ‰/í• ë‹¹ëŸ‰
- **`clone()`**: ì„¸ì…˜ ë³µì œ (ë©€í‹° ë¸Œëœì¹˜ ëŒ€í™”)
- **`append()`**: ì‘ë‹µ ì—†ì´ ë©”ì‹œì§€ ì¶”ê°€
- **`monitor`**: ë‹¤ìš´ë¡œë“œ ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§
- **`quotaoverflow` ì´ë²¤íŠ¸**: ì»¨í…ìŠ¤íŠ¸ ì°½ ì´ˆê³¼ ê°ì§€
- **`responseConstraint`**: JSON Schema ë˜ëŠ” RegExpë¡œ ì¶œë ¥ í˜•ì‹ ì œí•œ
- **ë©€í‹°ëª¨ë‹¬ ì…ë ¥**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ì§€ì›

### ì‚¬ìš© ì˜ˆì‹œ

```typescript
// 1. ê°€ìš©ì„± í™•ì¸
const availability = await LanguageModel.availability({
  expectedInputs: [{ type: "text" }],
  expectedOutputs: [{ type: "text" }]
})

// 2. íŒŒë¼ë¯¸í„° ì¡°íšŒ
const params = await LanguageModel.params()
// { defaultTemperature, maxTemperature, defaultTopK, maxTopK }

// 3. ì„¸ì…˜ ìƒì„±
const session = await LanguageModel.create({
  initialPrompts: [
    { role: "system", content: "You are a helpful assistant." }
  ],
  temperature: 0.8,
  topK: 10,
  expectedInputs: [{ type: "text", languages: ["ko", "en"] }],
  expectedOutputs: [{ type: "text", languages: ["ko"] }],
  signal: abortController.signal,
  monitor: (m) => {
    m.addEventListener("downloadprogress", (e) => {
      console.log(`Downloaded ${e.loaded * 100}%`)
    })
  }
})

// 4. í”„ë¡¬í”„íŠ¸ (ë¹„ìŠ¤íŠ¸ë¦¬ë°)
const response = await session.prompt("ì•ˆë…•í•˜ì„¸ìš”!")

// 5. í”„ë¡¬í”„íŠ¸ (ìŠ¤íŠ¸ë¦¬ë°)
const stream = session.promptStreaming("ê¸´ ì´ì•¼ê¸°ë¥¼ í•´ì£¼ì„¸ìš”.")
for await (const chunk of stream) {
  console.log(chunk)
}

// 6. êµ¬ì¡°í™”ëœ ì¶œë ¥
const result = await session.prompt("ì´ë©”ì¼ ìƒì„±í•´ì¤˜", {
  responseConstraint: {
    type: "object",
    properties: { email: { type: "string" } }
  }
})

// 7. ì„¸ì…˜ ì •ë³´
console.log(`${session.inputUsage}/${session.inputQuota} tokens used`)

// 8. ì •ë¦¬
session.destroy()
```
